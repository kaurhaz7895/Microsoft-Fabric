{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\n",
        "from pyspark.sql.functions import col, current_timestamp\n",
        "import random\n",
        "import string\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"InsertRandomData2SampleIngressTable\") \\\n",
        "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define the schema\n",
        "schema = StructType([\n",
        "    StructField(\"ID\", LongType(), False),\n",
        "    StructField(\"TimeIngress\", TimestampType(), True),\n",
        "    StructField(\"valOfIngress\", IntegerType(), True),\n",
        "    StructField(\"textIngress\", StringType(), True)\n",
        "])\n",
        "\n",
        "\n",
        "random_data = [(id, None, random.randint(1, 100), ''.join(random.choices(string.ascii_letters, k=50))) for id in range(11, 21)]\n",
        "random_df = spark.createDataFrame(random_data, schema=schema)\n",
        "\n",
        "random_df.write.format(\"delta\").mode(\"append\").save(\"abfss://1860xxxxxxxxxxxx99b92e1@onelake.dfs.fabric.microsoft.com/a574d1a3xxxxxxxx28f/Tables/SampleIngress\")\n",
        "\n",
        "spark.stop()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": "5938c4e6-282f-4eb1-aad6-98a8da8d1977",
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-12-16T16:03:40.8846641Z",
              "session_start_time": "2023-12-16T16:03:41.1359143Z",
              "execution_start_time": "2023-12-16T16:03:49.8042048Z",
              "execution_finish_time": "2023-12-16T16:03:58.5001692Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 7
                },
                "jobs": [
                  {
                    "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107",
                    "dataWritten": 0,
                    "dataRead": 4393,
                    "rowCount": 50,
                    "usageDescription": "",
                    "jobId": 15,
                    "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107",
                    "description": "Delta: Job group for statement 3:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\nfrom pyspark.sql.functions import col, current_timestamp\nimport random\nimport string\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateDeltaTableWithRandomData2\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n\nrandom_data = [(id, None, random.randint(1, 100), ''.join(random.choices(string.ascii_letters, k=50))) for id in range(11, 21)]\nrandom_df = spark.createDataFrame(random_data, schema=schema)\n\nrandom_df.write.format(\"delta\").mode(\"append\").save(\"abfss://1860beee-5b6a-48cc-9276-1a8d699b92e1@onelake.dfs.fabric.microsoft.com/a574d1a3-f10e-498e-9202-d95e1...: Compute snapshot for version: 3",
                    "submissionTime": "2023-12-16T16:03:56.027GMT",
                    "completionTime": "2023-12-16T16:03:56.078GMT",
                    "stageIds": [
                      21,
                      22,
                      23
                    ],
                    "jobGroup": "3",
                    "status": "SUCCEEDED",
                    "numTasks": 55,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 54,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 2,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107",
                    "dataWritten": 4393,
                    "dataRead": 4676,
                    "rowCount": 59,
                    "usageDescription": "",
                    "jobId": 14,
                    "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107",
                    "description": "Delta: Job group for statement 3:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\nfrom pyspark.sql.functions import col, current_timestamp\nimport random\nimport string\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateDeltaTableWithRandomData2\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n\nrandom_data = [(id, None, random.randint(1, 100), ''.join(random.choices(string.ascii_letters, k=50))) for id in range(11, 21)]\nrandom_df = spark.createDataFrame(random_data, schema=schema)\n\nrandom_df.write.format(\"delta\").mode(\"append\").save(\"abfss://1860beee-5b6a-48cc-9276-1a8d699b92e1@onelake.dfs.fabric.microsoft.com/a574d1a3-f10e-498e-9202-d95e1...: Compute snapshot for version: 3",
                    "submissionTime": "2023-12-16T16:03:54.972GMT",
                    "completionTime": "2023-12-16T16:03:56.000GMT",
                    "stageIds": [
                      19,
                      20
                    ],
                    "jobGroup": "3",
                    "status": "SUCCEEDED",
                    "numTasks": 54,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 50,
                    "numSkippedTasks": 4,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 50,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107",
                    "dataWritten": 4676,
                    "dataRead": 3729,
                    "rowCount": 18,
                    "usageDescription": "",
                    "jobId": 13,
                    "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107",
                    "description": "Delta: Job group for statement 3:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\nfrom pyspark.sql.functions import col, current_timestamp\nimport random\nimport string\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateDeltaTableWithRandomData2\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n\nrandom_data = [(id, None, random.randint(1, 100), ''.join(random.choices(string.ascii_letters, k=50))) for id in range(11, 21)]\nrandom_df = spark.createDataFrame(random_data, schema=schema)\n\nrandom_df.write.format(\"delta\").mode(\"append\").save(\"abfss://1860beee-5b6a-48cc-9276-1a8d699b92e1@onelake.dfs.fabric.microsoft.com/a574d1a3-f10e-498e-9202-d95e1...: Compute snapshot for version: 3",
                    "submissionTime": "2023-12-16T16:03:54.676GMT",
                    "completionTime": "2023-12-16T16:03:54.796GMT",
                    "stageIds": [
                      18
                    ],
                    "jobGroup": "3",
                    "status": "SUCCEEDED",
                    "numTasks": 4,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 4,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 4,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "toString at String.java:2951",
                    "dataWritten": 0,
                    "dataRead": 3729,
                    "rowCount": 9,
                    "usageDescription": "",
                    "jobId": 12,
                    "name": "toString at String.java:2951",
                    "description": "Job group for statement 3:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\nfrom pyspark.sql.functions import col, current_timestamp\nimport random\nimport string\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateDeltaTableWithRandomData2\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n\nrandom_data = [(id, None, random.randint(1, 100), ''.join(random.choices(string.ascii_letters, k=50))) for id in range(11, 21)]\nrandom_df = spark.createDataFrame(random_data, schema=schema)\n\nrandom_df.write.format(\"delta\").mode(\"append\").save(\"abfss://1860beee-5b6a-48cc-9276-1a8d699b92e1@onelake.dfs.fabric.microsoft.com/a574d1a3-f10e-498e-9202-d95e1...",
                    "submissionTime": "2023-12-16T16:03:54.176GMT",
                    "completionTime": "2023-12-16T16:03:54.279GMT",
                    "stageIds": [
                      17
                    ],
                    "jobGroup": "3",
                    "status": "SUCCEEDED",
                    "numTasks": 4,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 4,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 4,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "save at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 2877,
                    "dataRead": 1166,
                    "rowCount": 20,
                    "usageDescription": "",
                    "jobId": 11,
                    "name": "save at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 3:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\nfrom pyspark.sql.functions import col, current_timestamp\nimport random\nimport string\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateDeltaTableWithRandomData2\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n\nrandom_data = [(id, None, random.randint(1, 100), ''.join(random.choices(string.ascii_letters, k=50))) for id in range(11, 21)]\nrandom_df = spark.createDataFrame(random_data, schema=schema)\n\nrandom_df.write.format(\"delta\").mode(\"append\").save(\"abfss://1860beee-5b6a-48cc-9276-1a8d699b92e1@onelake.dfs.fabric.microsoft.com/a574d1a3-f10e-498e-9202-d95e1...",
                    "submissionTime": "2023-12-16T16:03:52.375GMT",
                    "completionTime": "2023-12-16T16:03:52.798GMT",
                    "stageIds": [
                      15,
                      16
                    ],
                    "jobGroup": "3",
                    "status": "SUCCEEDED",
                    "numTasks": 9,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 8,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "save at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 1166,
                    "dataRead": 0,
                    "rowCount": 10,
                    "usageDescription": "",
                    "jobId": 10,
                    "name": "save at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 3:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\nfrom pyspark.sql.functions import col, current_timestamp\nimport random\nimport string\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateDeltaTableWithRandomData2\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n\nrandom_data = [(id, None, random.randint(1, 100), ''.join(random.choices(string.ascii_letters, k=50))) for id in range(11, 21)]\nrandom_df = spark.createDataFrame(random_data, schema=schema)\n\nrandom_df.write.format(\"delta\").mode(\"append\").save(\"abfss://1860beee-5b6a-48cc-9276-1a8d699b92e1@onelake.dfs.fabric.microsoft.com/a574d1a3-f10e-498e-9202-d95e1...",
                    "submissionTime": "2023-12-16T16:03:51.185GMT",
                    "completionTime": "2023-12-16T16:03:52.334GMT",
                    "stageIds": [
                      14
                    ],
                    "jobGroup": "3",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "toString at String.java:2951",
                    "dataWritten": 0,
                    "dataRead": 2809,
                    "rowCount": 7,
                    "usageDescription": "",
                    "jobId": 9,
                    "name": "toString at String.java:2951",
                    "description": "Job group for statement 3:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\nfrom pyspark.sql.functions import col, current_timestamp\nimport random\nimport string\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateDeltaTableWithRandomData2\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n\nrandom_data = [(id, None, random.randint(1, 100), ''.join(random.choices(string.ascii_letters, k=50))) for id in range(11, 21)]\nrandom_df = spark.createDataFrame(random_data, schema=schema)\n\nrandom_df.write.format(\"delta\").mode(\"append\").save(\"abfss://1860beee-5b6a-48cc-9276-1a8d699b92e1@onelake.dfs.fabric.microsoft.com/a574d1a3-f10e-498e-9202-d95e1...",
                    "submissionTime": "2023-12-16T16:03:50.759GMT",
                    "completionTime": "2023-12-16T16:03:51.072GMT",
                    "stageIds": [
                      13
                    ],
                    "jobGroup": "3",
                    "status": "SUCCEEDED",
                    "numTasks": 3,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 3,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 3,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "850b4413-2c81-41e3-b52b-b9fe4ffd524f"
            },
            "text/plain": "StatementMeta(, 5938c4e6-282f-4eb1-aad6-98a8da8d1977, 3, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "advisor": {
          "adviceMetadata": "{\"artifactId\":\"e8aef27e-b2b6-498a-a82e-c76699de759a\",\"activityId\":\"5938c4e6-282f-4eb1-aad6-98a8da8d1977\",\"applicationId\":\"application_1702742426991_0001\",\"jobGroupId\":\"3\",\"advices\":{\"info\":1}}"
        }
      },
      "id": "04c096aa-85aa-4ea6-9d4c-b819bc2ebc60"
    }
  ],
  "metadata": {
    "save_output": true,
    "microsoft": {
      "host": {},
      "language": "python"
    },
    "notebook_environment": {},
    "widgets": {},
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "trident": {
      "lakehouse": {
        "known_lakehouses": [
          {
            "id": "a574d1a3-f10e-498e-9202-d95e18c7128f"
          }
        ],
        "default_lakehouse": "a574d1a3-f10e-498e-9202-d95e18c7128f",
        "default_lakehouse_name": "Advent2023",
        "default_lakehouse_workspace_id": "1860beee-5b6a-48cc-9276-1a8d699b92e1"
      }
    },
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "enableDebugMode": false,
        "conf": {}
      }
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}