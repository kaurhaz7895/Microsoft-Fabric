{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Using delta tables in data warehouse\n",
        "\n",
        "\n",
        "We will create a delta table using spark.\n",
        "And we will schedule to populate the table with random data every minute; imitating data ingest."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "0110368a-eb59-47c4-89ee-258bd6aa37dc"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CreateEmptyDeltaTable\") \\\n",
        "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define the schema\n",
        "schema = StructType([\n",
        "    StructField(\"ID\", LongType(), False),\n",
        "    StructField(\"TimeIngress\", TimestampType(), True),\n",
        "    StructField(\"valOfIngress\", IntegerType(), True),\n",
        "    StructField(\"textIngress\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Create an empty DataFrame with the specified schema\n",
        "empty_df = spark.createDataFrame([], schema=schema)\n",
        "\n",
        "\n",
        "\n",
        "# Write the empty DataFrame as a Delta table\n",
        "empty_df.write.format(\"delta\").mode(\"overwrite\").save(\"abfss://1860xxxxxxxxxxx699b92e1@onelake.dfs.fabric.microsoft.com/a574xxxxxxxxxx18c7128f/Tables/SampleIngress\")\n",
        "\n",
        "spark.stop()\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": "ea423724-f942-42cb-af3b-e247a854e1d3",
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-12-16T15:31:49.1508293Z",
              "session_start_time": null,
              "execution_start_time": "2023-12-16T15:31:49.5310486Z",
              "execution_finish_time": "2023-12-16T15:32:17.6483537Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 7,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107",
                    "dataWritten": 0,
                    "dataRead": 4354,
                    "rowCount": 50,
                    "usageDescription": "",
                    "jobId": 15,
                    "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107",
                    "description": "Delta: Job group for statement 7:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateEmptyDeltaTable\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n# Create an empty DataFrame with the specified schema\nempty_df = spark.createDataFrame([], schema=schema)\n\n\n\n# Write the empty DataFrame as a Delta table\nempty_df.write.format(\"delta\").mode(\"overwrite\").save(\"abfss://1860beee-5b6a-48cc-9276-1a8d699b92e1@onelake.dfs.fabric.microsoft.com/a574d1a3-f10e-498e-9202-d95e18c7128f/Tables/SampleIngress\")\n\n# Alternatively, if you want to append to an existing Delta table\n# empty_df.write.format(\"delta\")....: Compute snapshot for version: 0",
                    "submissionTime": "2023-12-16T15:32:14.531GMT",
                    "completionTime": "2023-12-16T15:32:14.624GMT",
                    "stageIds": [
                      19,
                      20,
                      21
                    ],
                    "jobGroup": "7",
                    "status": "SUCCEEDED",
                    "numTasks": 52,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 51,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 2,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107",
                    "dataWritten": 4354,
                    "dataRead": 953,
                    "rowCount": 53,
                    "usageDescription": "",
                    "jobId": 14,
                    "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107",
                    "description": "Delta: Job group for statement 7:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateEmptyDeltaTable\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n# Create an empty DataFrame with the specified schema\nempty_df = spark.createDataFrame([], schema=schema)\n\n\n\n# Write the empty DataFrame as a Delta table\nempty_df.write.format(\"delta\").mode(\"overwrite\").save(\"abfss://1860beee-5b6a-48cc-9276-1a8d699b92e1@onelake.dfs.fabric.microsoft.com/a574d1a3-f10e-498e-9202-d95e18c7128f/Tables/SampleIngress\")\n\n# Alternatively, if you want to append to an existing Delta table\n# empty_df.write.format(\"delta\")....: Compute snapshot for version: 0",
                    "submissionTime": "2023-12-16T15:32:12.591GMT",
                    "completionTime": "2023-12-16T15:32:14.507GMT",
                    "stageIds": [
                      17,
                      18
                    ],
                    "jobGroup": "7",
                    "status": "SUCCEEDED",
                    "numTasks": 51,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 50,
                    "numSkippedTasks": 1,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 50,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107",
                    "dataWritten": 953,
                    "dataRead": 972,
                    "rowCount": 6,
                    "usageDescription": "",
                    "jobId": 13,
                    "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107",
                    "description": "Delta: Job group for statement 7:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateEmptyDeltaTable\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n# Create an empty DataFrame with the specified schema\nempty_df = spark.createDataFrame([], schema=schema)\n\n\n\n# Write the empty DataFrame as a Delta table\nempty_df.write.format(\"delta\").mode(\"overwrite\").save(\"abfss://1860beee-5b6a-48cc-9276-1a8d699b92e1@onelake.dfs.fabric.microsoft.com/a574d1a3-f10e-498e-9202-d95e18c7128f/Tables/SampleIngress\")\n\n# Alternatively, if you want to append to an existing Delta table\n# empty_df.write.format(\"delta\")....: Compute snapshot for version: 0",
                    "submissionTime": "2023-12-16T15:32:11.846GMT",
                    "completionTime": "2023-12-16T15:32:12.408GMT",
                    "stageIds": [
                      16
                    ],
                    "jobGroup": "7",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "toString at String.java:2951",
                    "dataWritten": 0,
                    "dataRead": 972,
                    "rowCount": 3,
                    "usageDescription": "",
                    "jobId": 12,
                    "name": "toString at String.java:2951",
                    "description": "Job group for statement 7:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateEmptyDeltaTable\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n# Create an empty DataFrame with the specified schema\nempty_df = spark.createDataFrame([], schema=schema)\n\n\n\n# Write the empty DataFrame as a Delta table\nempty_df.write.format(\"delta\").mode(\"overwrite\").save(\"abfss://1860beee-5b6a-48cc-9276-1a8d699b92e1@onelake.dfs.fabric.microsoft.com/a574d1a3-f10e-498e-9202-d95e18c7128f/Tables/SampleIngress\")\n\n# Alternatively, if you want to append to an existing Delta table\n# empty_df.write.format(\"delta\")....",
                    "submissionTime": "2023-12-16T15:32:10.977GMT",
                    "completionTime": "2023-12-16T15:32:11.625GMT",
                    "stageIds": [
                      15
                    ],
                    "jobGroup": "7",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "Job group for statement 7:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateEmptyDeltaTable\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n# Create an empty DataFrame with the specified schema\nempty_df = spark.createDataFrame([], schema=schema)\n\n\n\n# Write the empty DataFrame as a Delta table\nempty_df.write.format(\"delta\").mode(\"overwrite\").save(\"abfss://1860beee-5b6a-48cc-9276-1a8d699b92e1@onelake.dfs.fabric.microsoft.com/a574d1a3-f10e-498e-9202-d95e18c7128f/Tables/SampleIngress\")\n\n# Alternatively, if you want to append to an existing Delta table\n# empty_df.write.format(\"delta\")....",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 11,
                    "name": "",
                    "description": "Job group for statement 7:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateEmptyDeltaTable\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n# Create an empty DataFrame with the specified schema\nempty_df = spark.createDataFrame([], schema=schema)\n\n\n\n# Write the empty DataFrame as a Delta table\nempty_df.write.format(\"delta\").mode(\"overwrite\").save(\"abfss://1860beee-5b6a-48cc-9276-1a8d699b92e1@onelake.dfs.fabric.microsoft.com/a574d1a3-f10e-498e-9202-d95e18c7128f/Tables/SampleIngress\")\n\n# Alternatively, if you want to append to an existing Delta table\n# empty_df.write.format(\"delta\")....",
                    "submissionTime": "2023-12-16T15:32:08.862GMT",
                    "completionTime": "2023-12-16T15:32:08.862GMT",
                    "stageIds": [],
                    "jobGroup": "7",
                    "status": "SUCCEEDED",
                    "numTasks": 0,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 0,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 0,
                    "numActiveStages": 0,
                    "numCompletedStages": 0,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "save at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 732,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 10,
                    "name": "save at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 7:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateEmptyDeltaTable\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n# Create an empty DataFrame with the specified schema\nempty_df = spark.createDataFrame([], schema=schema)\n\n\n\n# Write the empty DataFrame as a Delta table\nempty_df.write.format(\"delta\").mode(\"overwrite\").save(\"abfss://1860beee-5b6a-48cc-9276-1a8d699b92e1@onelake.dfs.fabric.microsoft.com/a574d1a3-f10e-498e-9202-d95e18c7128f/Tables/SampleIngress\")\n\n# Alternatively, if you want to append to an existing Delta table\n# empty_df.write.format(\"delta\")....",
                    "submissionTime": "2023-12-16T15:31:59.167GMT",
                    "completionTime": "2023-12-16T15:32:08.704GMT",
                    "stageIds": [
                      14
                    ],
                    "jobGroup": "7",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "save at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 9,
                    "name": "save at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 7:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateEmptyDeltaTable\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n# Create an empty DataFrame with the specified schema\nempty_df = spark.createDataFrame([], schema=schema)\n\n\n\n# Write the empty DataFrame as a Delta table\nempty_df.write.format(\"delta\").mode(\"overwrite\").save(\"abfss://1860beee-5b6a-48cc-9276-1a8d699b92e1@onelake.dfs.fabric.microsoft.com/a574d1a3-f10e-498e-9202-d95e18c7128f/Tables/SampleIngress\")\n\n# Alternatively, if you want to append to an existing Delta table\n# empty_df.write.format(\"delta\")....",
                    "submissionTime": "2023-12-16T15:31:49.773GMT",
                    "completionTime": "2023-12-16T15:31:59.125GMT",
                    "stageIds": [
                      13
                    ],
                    "jobGroup": "7",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "5973a593-2286-4ff7-9965-7b72a0bb904c"
            },
            "text/plain": "StatementMeta(, ea423724-f942-42cb-af3b-e247a854e1d3, 7, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "advisor": {
          "adviceMetadata": "{\"artifactId\":\"05a1b459-c0f5-4e8e-8a27-8c72a356eb6b\",\"activityId\":\"ea423724-f942-42cb-af3b-e247a854e1d3\",\"applicationId\":\"application_1702740078032_0001\",\"jobGroupId\":\"7\",\"advices\":{\"info\":1}}"
        }
      },
      "id": "f91e56c9-2772-4bad-af17-b76ff0153474"
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we can create a sample data to be inserted (ingressed) into delta table"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "8675debc-bcec-40b3-abb0-818afe32f905"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\n",
        "from pyspark.sql.functions import col, current_timestamp\n",
        "import random\n",
        "import string\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CreateDeltaTableWithRandomData2\") \\\n",
        "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define the schema\n",
        "schema = StructType([\n",
        "    StructField(\"ID\", LongType(), False),\n",
        "    StructField(\"TimeIngress\", TimestampType(), True),\n",
        "    StructField(\"valOfIngress\", IntegerType(), True),\n",
        "    StructField(\"textIngress\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Generate and add random rows of data\n",
        "random_data = [(id, None, random.randint(1, 100), ''.join(random.choices(string.ascii_letters, k=50))) for id in range(11, 21)]\n",
        "random_df = spark.createDataFrame(random_data, schema=schema)\n",
        "\n",
        "# Append the random data to the Delta table\n",
        "random_df.write.format(\"delta\").mode(\"append\").save(\"abfss://1860bxxxxxxxx699b92e1@onelake.dfs.fabric.microsoft.com/a574d1xxxxxxxxx7128f/Tables/SampleIngress\")\n",
        "\n",
        "# Show the updated Delta table\n",
        "delta_table = spark.read.format(\"delta\").load(\"abfss://1860beeexxxxxxxxxx9b92e1@onelake.dfs.fabric.microsoft.com/a574d1xxxxxxxxx7128f/Tables/SampleIngress\")\n",
        "delta_table.show()\n",
        "\n",
        "spark.stop()\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": "362ebd5c-404c-41d0-99a7-b7ba0435382c",
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-12-16T16:01:30.3593208Z",
              "session_start_time": null,
              "execution_start_time": "2023-12-16T16:01:30.8852427Z",
              "execution_finish_time": "2023-12-16T16:02:01.0111024Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "FAILED": 0,
                  "SUCCEEDED": 10
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 1014,
                    "rowCount": 10,
                    "usageDescription": "",
                    "jobId": 18,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 5:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\nfrom pyspark.sql.functions import col, current_timestamp\nimport random\nimport string\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateDeltaTableWithRandomData2\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n# Create an empty DataFrame with the specified schema\n# mpty_df = spark.createDataFrame([], schema=schema)\n\n# Write the empty DataFrame as a Delta table\n#empty_df.write.format(\"delta\").mode(\"overwrite\").save(\"/path/to/SampleIngress\")\n\n# Generate and add random rows of data\nrandom_data = [(id, None, random.randint(1, 100), ''.join(random.choices(strin...",
                    "submissionTime": "2023-12-16T16:01:59.073GMT",
                    "completionTime": "2023-12-16T16:01:59.146GMT",
                    "stageIds": [
                      27
                    ],
                    "jobGroup": "5",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 2320,
                    "rowCount": 10,
                    "usageDescription": "",
                    "jobId": 17,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 5:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\nfrom pyspark.sql.functions import col, current_timestamp\nimport random\nimport string\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateDeltaTableWithRandomData2\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n# Create an empty DataFrame with the specified schema\n# mpty_df = spark.createDataFrame([], schema=schema)\n\n# Write the empty DataFrame as a Delta table\n#empty_df.write.format(\"delta\").mode(\"overwrite\").save(\"/path/to/SampleIngress\")\n\n# Generate and add random rows of data\nrandom_data = [(id, None, random.randint(1, 100), ''.join(random.choices(strin...",
                    "submissionTime": "2023-12-16T16:01:58.524GMT",
                    "completionTime": "2023-12-16T16:01:59.067GMT",
                    "stageIds": [
                      26
                    ],
                    "jobGroup": "5",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107",
                    "dataWritten": 0,
                    "dataRead": 2229,
                    "rowCount": 4,
                    "usageDescription": "",
                    "jobId": 16,
                    "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107",
                    "description": "Delta: Job group for statement 5:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\nfrom pyspark.sql.functions import col, current_timestamp\nimport random\nimport string\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateDeltaTableWithRandomData2\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n# Create an empty DataFrame with the specified schema\n# mpty_df = spark.createDataFrame([], schema=schema)\n\n# Write the empty DataFrame as a Delta table\n#empty_df.write.format(\"delta\").mode(\"overwrite\").save(\"/path/to/SampleIngress\")\n\n# Generate and add random rows of data\nrandom_data = [(id, None, random.randint(1, 100), ''.join(random.choices(strin...: Filtering files for query",
                    "submissionTime": "2023-12-16T16:01:57.839GMT",
                    "completionTime": "2023-12-16T16:01:58.386GMT",
                    "stageIds": [
                      24,
                      25
                    ],
                    "jobGroup": "5",
                    "status": "SUCCEEDED",
                    "numTasks": 53,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 50,
                    "numSkippedTasks": 3,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 50,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107",
                    "dataWritten": 0,
                    "dataRead": 4380,
                    "rowCount": 50,
                    "usageDescription": "",
                    "jobId": 15,
                    "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107",
                    "description": "Delta: Job group for statement 5:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\nfrom pyspark.sql.functions import col, current_timestamp\nimport random\nimport string\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateDeltaTableWithRandomData2\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n# Create an empty DataFrame with the specified schema\n# mpty_df = spark.createDataFrame([], schema=schema)\n\n# Write the empty DataFrame as a Delta table\n#empty_df.write.format(\"delta\").mode(\"overwrite\").save(\"/path/to/SampleIngress\")\n\n# Generate and add random rows of data\nrandom_data = [(id, None, random.randint(1, 100), ''.join(random.choices(strin...: Compute snapshot for version: 2",
                    "submissionTime": "2023-12-16T16:01:57.324GMT",
                    "completionTime": "2023-12-16T16:01:57.418GMT",
                    "stageIds": [
                      21,
                      22,
                      23
                    ],
                    "jobGroup": "5",
                    "status": "SUCCEEDED",
                    "numTasks": 54,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 53,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 2,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107",
                    "dataWritten": 4380,
                    "dataRead": 3431,
                    "rowCount": 57,
                    "usageDescription": "",
                    "jobId": 14,
                    "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107",
                    "description": "Delta: Job group for statement 5:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\nfrom pyspark.sql.functions import col, current_timestamp\nimport random\nimport string\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateDeltaTableWithRandomData2\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n# Create an empty DataFrame with the specified schema\n# mpty_df = spark.createDataFrame([], schema=schema)\n\n# Write the empty DataFrame as a Delta table\n#empty_df.write.format(\"delta\").mode(\"overwrite\").save(\"/path/to/SampleIngress\")\n\n# Generate and add random rows of data\nrandom_data = [(id, None, random.randint(1, 100), ''.join(random.choices(strin...: Compute snapshot for version: 2",
                    "submissionTime": "2023-12-16T16:01:55.336GMT",
                    "completionTime": "2023-12-16T16:01:57.298GMT",
                    "stageIds": [
                      19,
                      20
                    ],
                    "jobGroup": "5",
                    "status": "SUCCEEDED",
                    "numTasks": 53,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 50,
                    "numSkippedTasks": 3,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 50,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107",
                    "dataWritten": 3431,
                    "dataRead": 2809,
                    "rowCount": 14,
                    "usageDescription": "",
                    "jobId": 13,
                    "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107",
                    "description": "Delta: Job group for statement 5:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\nfrom pyspark.sql.functions import col, current_timestamp\nimport random\nimport string\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateDeltaTableWithRandomData2\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n# Create an empty DataFrame with the specified schema\n# mpty_df = spark.createDataFrame([], schema=schema)\n\n# Write the empty DataFrame as a Delta table\n#empty_df.write.format(\"delta\").mode(\"overwrite\").save(\"/path/to/SampleIngress\")\n\n# Generate and add random rows of data\nrandom_data = [(id, None, random.randint(1, 100), ''.join(random.choices(strin...: Compute snapshot for version: 2",
                    "submissionTime": "2023-12-16T16:01:54.627GMT",
                    "completionTime": "2023-12-16T16:01:55.178GMT",
                    "stageIds": [
                      18
                    ],
                    "jobGroup": "5",
                    "status": "SUCCEEDED",
                    "numTasks": 3,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 3,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 3,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "toString at String.java:2951",
                    "dataWritten": 0,
                    "dataRead": 2809,
                    "rowCount": 7,
                    "usageDescription": "",
                    "jobId": 12,
                    "name": "toString at String.java:2951",
                    "description": "Job group for statement 5:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\nfrom pyspark.sql.functions import col, current_timestamp\nimport random\nimport string\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateDeltaTableWithRandomData2\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n# Create an empty DataFrame with the specified schema\n# mpty_df = spark.createDataFrame([], schema=schema)\n\n# Write the empty DataFrame as a Delta table\n#empty_df.write.format(\"delta\").mode(\"overwrite\").save(\"/path/to/SampleIngress\")\n\n# Generate and add random rows of data\nrandom_data = [(id, None, random.randint(1, 100), ''.join(random.choices(strin...",
                    "submissionTime": "2023-12-16T16:01:54.296GMT",
                    "completionTime": "2023-12-16T16:01:54.369GMT",
                    "stageIds": [
                      17
                    ],
                    "jobGroup": "5",
                    "status": "SUCCEEDED",
                    "numTasks": 3,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 3,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 3,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "save at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 2879,
                    "dataRead": 1165,
                    "rowCount": 20,
                    "usageDescription": "",
                    "jobId": 11,
                    "name": "save at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 5:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\nfrom pyspark.sql.functions import col, current_timestamp\nimport random\nimport string\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateDeltaTableWithRandomData2\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n# Create an empty DataFrame with the specified schema\n# mpty_df = spark.createDataFrame([], schema=schema)\n\n# Write the empty DataFrame as a Delta table\n#empty_df.write.format(\"delta\").mode(\"overwrite\").save(\"/path/to/SampleIngress\")\n\n# Generate and add random rows of data\nrandom_data = [(id, None, random.randint(1, 100), ''.join(random.choices(strin...",
                    "submissionTime": "2023-12-16T16:01:45.946GMT",
                    "completionTime": "2023-12-16T16:01:52.898GMT",
                    "stageIds": [
                      15,
                      16
                    ],
                    "jobGroup": "5",
                    "status": "SUCCEEDED",
                    "numTasks": 9,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 8,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "save at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 1165,
                    "dataRead": 0,
                    "rowCount": 10,
                    "usageDescription": "",
                    "jobId": 10,
                    "name": "save at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 5:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\nfrom pyspark.sql.functions import col, current_timestamp\nimport random\nimport string\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateDeltaTableWithRandomData2\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n# Create an empty DataFrame with the specified schema\n# mpty_df = spark.createDataFrame([], schema=schema)\n\n# Write the empty DataFrame as a Delta table\n#empty_df.write.format(\"delta\").mode(\"overwrite\").save(\"/path/to/SampleIngress\")\n\n# Generate and add random rows of data\nrandom_data = [(id, None, random.randint(1, 100), ''.join(random.choices(strin...",
                    "submissionTime": "2023-12-16T16:01:39.390GMT",
                    "completionTime": "2023-12-16T16:01:45.907GMT",
                    "stageIds": [
                      14
                    ],
                    "jobGroup": "5",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "toString at String.java:2951",
                    "dataWritten": 0,
                    "dataRead": 1890,
                    "rowCount": 5,
                    "usageDescription": "",
                    "jobId": 9,
                    "name": "toString at String.java:2951",
                    "description": "Job group for statement 5:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, StringType\nfrom pyspark.sql.functions import col, current_timestamp\nimport random\nimport string\n\n# Create a Spark session\nspark = SparkSession.builder     .appName(\"CreateDeltaTableWithRandomData2\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")     .getOrCreate()\n\n# Define the schema\nschema = StructType([\n    StructField(\"ID\", LongType(), False),\n    StructField(\"TimeIngress\", TimestampType(), True),\n    StructField(\"valOfIngress\", IntegerType(), True),\n    StructField(\"textIngress\", StringType(), True)\n])\n\n# Create an empty DataFrame with the specified schema\n# mpty_df = spark.createDataFrame([], schema=schema)\n\n# Write the empty DataFrame as a Delta table\n#empty_df.write.format(\"delta\").mode(\"overwrite\").save(\"/path/to/SampleIngress\")\n\n# Generate and add random rows of data\nrandom_data = [(id, None, random.randint(1, 100), ''.join(random.choices(strin...",
                    "submissionTime": "2023-12-16T16:01:31.196GMT",
                    "completionTime": "2023-12-16T16:01:39.299GMT",
                    "stageIds": [
                      13
                    ],
                    "jobGroup": "5",
                    "status": "SUCCEEDED",
                    "numTasks": 2,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "62aa4f7a-43d1-4e16-8b19-764c32cd7bbf"
            },
            "text/plain": "StatementMeta(, 362ebd5c-404c-41d0-99a7-b7ba0435382c, 5, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------+------------+--------------------+\n",
            "| ID|TimeIngress|valOfIngress|         textIngress|\n",
            "+---+-----------+------------+--------------------+\n",
            "| 11|       null|          24|SHqmnJsxKydOTZpKg...|\n",
            "| 12|       null|           8|wBIkiaCBdQsZKhNfm...|\n",
            "| 13|       null|          16|cZOFcgQtnsXrfGFtq...|\n",
            "| 14|       null|          67|EuJryztxHbHVUmkhW...|\n",
            "| 15|       null|           3|jNdlfQuUUpASGBgss...|\n",
            "| 16|       null|          42|zDHHlaQlFdhBRjXPA...|\n",
            "| 17|       null|          44|kXVJNHXEVkUgDYkRo...|\n",
            "| 18|       null|          41|MHFPhtckfWfuBRRCH...|\n",
            "| 19|       null|          55|lzOToCPWfWDqAQawq...|\n",
            "| 20|       null|          69|iZweTDTHdEThrxomd...|\n",
            "| 11|       null|          11|oyttxDujEVvOamNNP...|\n",
            "| 12|       null|          36|CdXJbJQkWpVGpZkMs...|\n",
            "| 13|       null|          86|yQUdgISLrfGPFRNLZ...|\n",
            "| 14|       null|          52|zSsAqKKvbSDnzXity...|\n",
            "| 15|       null|          17|KRpsNvegBcCQNgbLY...|\n",
            "| 16|       null|          41|BeJnloqthCOOCcNgS...|\n",
            "| 17|       null|          20|PwoYRgbwfLsqkUjHE...|\n",
            "| 18|       null|          17|YtOfALIetpGrPNHxY...|\n",
            "| 19|       null|          46|xtjWtHdjxTdvKwMOi...|\n",
            "| 20|       null|          30|JWCHbYbWwYijweKBZ...|\n",
            "+---+-----------+------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "advisor": {
          "adviceMetadata": "{\"artifactId\":\"05a1b459-c0f5-4e8e-8a27-8c72a356eb6b\",\"activityId\":\"362ebd5c-404c-41d0-99a7-b7ba0435382c\",\"applicationId\":\"application_1702741449593_0001\",\"jobGroupId\":\"5\",\"advices\":{\"info\":1}}"
        }
      },
      "id": "57472d65-2162-493a-9cf2-067ab1d0e860"
    }
  ],
  "metadata": {
    "save_output": true,
    "microsoft": {
      "host": {},
      "language": "python"
    },
    "notebook_environment": {},
    "widgets": {},
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "trident": {
      "lakehouse": {
        "known_lakehouses": [
          {
            "id": "a574d1a3-f10e-498e-9202-d95e18c7128f"
          }
        ],
        "default_lakehouse": "a574d1a3-f10e-498e-9202-d95e18c7128f",
        "default_lakehouse_name": "Advent2023",
        "default_lakehouse_workspace_id": "1860beee-5b6a-48cc-9276-1a8d699b92e1"
      }
    },
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "enableDebugMode": false,
        "conf": {}
      }
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}